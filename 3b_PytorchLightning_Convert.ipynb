{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "from torchvision import transforms\n",
    "import os\n",
    "from pytorch_lightning.loggers.tensorboard import TensorBoardLogger\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test.json', 'valid.json', 'train.csv', 'test.csv', 'train.json', 'segmentation', '.ipynb_checkpoints', 'test', 'train']\n"
     ]
    }
   ],
   "source": [
    "m_path = '/home/tungnguyendinh/.fastai/data/pascal_2007/'\n",
    "print(os.listdir(m_path))\n",
    "\n",
    "#Define classes and encode/decode functions\n",
    "\n",
    "VOC_CLASSES = (\n",
    "    'aeroplane', 'bicycle', 'bird', 'boat',\n",
    "    'bottle', 'bus', 'car', 'cat', 'chair',\n",
    "    'cow', 'diningtable', 'dog', 'horse',\n",
    "    'motorbike', 'person', 'pottedplant',\n",
    "    'sheep', 'sofa', 'train', 'tvmonitor'\n",
    ")\n",
    "\n",
    "def encode_label(labels, classes = VOC_CLASSES):\n",
    "    target = torch.zeros(len(classes))\n",
    "    for l in labels:\n",
    "        idx = classes.index(l)\n",
    "        target[idx] = 1\n",
    "    return target\n",
    "\n",
    "def decode_target(target, threshold = 0.5, classes = VOC_CLASSES):\n",
    "    result = []\n",
    "    for i, x in enumerate(target):\n",
    "        if (x >= threshold):\n",
    "            result.append(classes[i])\n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.io import read_image\n",
    "#Create Custom dataset class for Pascal_2007\n",
    "class PascalDataset(Dataset):\n",
    "    def __init__(self, img_dir, annotations_file, transform=None, target_transform=None, is_train = False, is_valid = False):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        if is_train:\n",
    "            self.img_labels = self.img_labels[~self.img_labels[\"is_valid\"]]\n",
    "        if is_valid:\n",
    "            self.img_labels = self.img_labels[self.img_labels[\"is_valid\"]]\n",
    "            \n",
    "                \n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = read_image(img_path)\n",
    "        label = self.img_labels.iloc[idx, 1].split(\" \")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, encode_label(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PascalVOCDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, batch_size, data_dir):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.data_dir = data_dir\n",
    "        self.mean = [0.457342265910642, 0.4387686270106377, 0.4073427106250871]\n",
    "        self.std = [0.26753769276329037, 0.2638145880487105, 0.2776826934044154]  \n",
    "        self.train_transform = transforms.Compose([\n",
    "                transforms.ToPILImage(),\n",
    "                transforms.Resize((300, 300)),\n",
    "                transforms.RandomChoice([\n",
    "                                        transforms.ColorJitter(brightness=(0.9, 1.1)),\n",
    "                                        transforms.RandomGrayscale(p = 0.25)\n",
    "                                        ]),\n",
    "                transforms.RandomHorizontalFlip(p = 0.25),\n",
    "                transforms.RandomRotation(25),\n",
    "                transforms.ToTensor(), \n",
    "                transforms.Normalize(mean = self.mean, std = self.std),\n",
    "                ])\n",
    "        self.eval_transform = transforms.Compose([\n",
    "                    transforms.ToPILImage(),\n",
    "                    transforms.Resize((300, 300)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean = self.mean, std = self.std)\n",
    "                   ])\n",
    "        \n",
    "    def setup(self):\n",
    "        stage = \"train\"\n",
    "        self.voc_train = PascalDataset(f\"{self.data_dir}/{stage}\", f\"{self.data_dir}/{stage}.csv\", transform=self.train_transform, is_train= True)\n",
    "        self.voc_val = PascalDataset(f\"{self.data_dir}/{stage}\", f\"{self.data_dir}/{stage}.csv\", transform=self.eval_transform, is_valid = True) \n",
    "        stage = \"test\"                       \n",
    "        self.voc_test = PascalDataset(f\"{self.data_dir}/{stage}\", f\"{self.data_dir}/{stage}.csv\", transform=self.eval_transform)\n",
    "            \n",
    "    def train_dataloader(self):\n",
    "        self.setup()\n",
    "        return DataLoader(self.voc_train, self.batch_size)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.voc_val, self.batch_size)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.voc_test, self.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "voc = PascalVOCDataModule(BATCH_SIZE, m_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def Accuracy_n_HammingScore(y_pred, y_true):\n",
    "    y_pred = torch.round(y_pred)\n",
    "    label = np.asarray(y_true.cpu())\n",
    "    prob = np.asarray(y_pred.cpu())\n",
    "    batch_size = label.shape[0]\n",
    "    acc = 0\n",
    "    hl = 0\n",
    "    for i in range(batch_size):\n",
    "        acc += accuracy_score(prob[i], label[i], normalize=True)\n",
    "        hl += sum(torch.logical_and(\n",
    "            y_true[i], y_pred[i])) / sum(torch.logical_or(y_true[i], y_pred[i]))\n",
    "    acc /= batch_size\n",
    "    hl /= batch_size\n",
    "    acc = torch.Tensor([acc])\n",
    "    return acc, hl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.utilities.types import EPOCH_OUTPUT\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, i_downsample=None, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels,\n",
    "                               kernel_size=1, stride=1, padding=0)\n",
    "        self.batch_norm1 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels,\n",
    "                               kernel_size=3, stride=stride, padding=1)\n",
    "        self.batch_norm2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            out_channels, out_channels*self.expansion, kernel_size=1, stride=1, padding=0)\n",
    "        self.batch_norm3 = nn.BatchNorm2d(out_channels*self.expansion)\n",
    "\n",
    "        self.i_downsample = i_downsample\n",
    "        self.stride = stride\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x.clone()\n",
    "        x = self.relu(self.batch_norm1(self.conv1(x)))\n",
    "\n",
    "        x = self.relu(self.batch_norm2(self.conv2(x)))\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.batch_norm3(x)\n",
    "\n",
    "        # downsample if needed\n",
    "        if self.i_downsample is not None:\n",
    "            identity = self.i_downsample(identity)\n",
    "        # add identity\n",
    "        x += identity\n",
    "        x = self.relu(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, i_downsample=None, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels, out_channels, kernel_size=3, padding=1, stride=stride, bias=False)\n",
    "        self.batch_norm1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels,\n",
    "                               kernel_size=3, padding=1, stride=stride, bias=False)\n",
    "        self.batch_norm2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.i_downsample = i_downsample\n",
    "        self.stride = stride\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x.clone()\n",
    "\n",
    "        x = self.relu(self.batch_norm2(self.conv1(x)))\n",
    "        x = self.batch_norm2(self.conv2(x))\n",
    "\n",
    "        if self.i_downsample is not None:\n",
    "            identity = self.i_downsample(identity)\n",
    "        x += identity\n",
    "        x = self.relu(x)\n",
    "        return\n",
    "\n",
    "\n",
    "class ResNet(pl.LightningModule):\n",
    "    def __init__(self, ResBlock, layer_list, num_classes, num_channels=3):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
    "        self.in_channels = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            num_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.batch_norm1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = self._make_layer(ResBlock, layer_list[0], planes=64)\n",
    "        self.layer2 = self._make_layer(\n",
    "            ResBlock, layer_list[1], planes=128, stride=2)\n",
    "        self.layer3 = self._make_layer(\n",
    "            ResBlock, layer_list[2], planes=256, stride=2)\n",
    "        self.layer4 = self._make_layer(\n",
    "            ResBlock, layer_list[3], planes=512, stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512*ResBlock.expansion, num_classes)\n",
    "    \n",
    "    def _make_layer(self, ResBlock, blocks, planes, stride=1):\n",
    "        ii_downsample = None\n",
    "        layers = []\n",
    "\n",
    "        if stride != 1 or self.in_channels != planes*ResBlock.expansion:\n",
    "            ii_downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_channels, planes*ResBlock.expansion,\n",
    "                          kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm2d(planes*ResBlock.expansion)\n",
    "            )\n",
    "\n",
    "        layers.append(ResBlock(self.in_channels, planes,\n",
    "                      i_downsample=ii_downsample, stride=stride))\n",
    "        self.in_channels = planes*ResBlock.expansion\n",
    "\n",
    "        for i in range(blocks-1):\n",
    "            layers.append(ResBlock(self.in_channels, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.batch_norm1(self.conv1(x)))\n",
    "        x = self.max_pool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.SGD(self.parameters(), lr=1e-3, momentum=0.9)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 12, eta_min=0, last_epoch=-1)\n",
    "        return [optimizer], [scheduler]\n",
    "    \n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        X, y = train_batch\n",
    "\n",
    "        pred = self.forward(X)\n",
    "        loss = self.loss_fn(pred, y)\n",
    "        tmp_pred = nn.Sigmoid()(pred).detach()\n",
    "        tmp_y    = y.detach()\n",
    "        acc, hl = Accuracy_n_HammingScore(tmp_pred, tmp_y)\n",
    "        batch_dictionary = {\n",
    "            \"loss\" : loss,\n",
    "            \"acc\"  : acc,\n",
    "            \"hl\"   : hl\n",
    "        }\n",
    "        if (batch_idx % 50 == 0):    \n",
    "            print(f\"loss: {loss:>7f}, training accuracy: {100*acc.item():>0.1f}%, hamming score: {100*hl.item():>0.1f}% Batch: {batch_idx}\")       \n",
    "             \n",
    "        return batch_dictionary\n",
    "    \n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        X, y = val_batch\n",
    "        pred = self.forward(X)\n",
    "        loss = self.loss_fn(pred, y)\n",
    "        pred = nn.Sigmoid()(pred)\n",
    "        acc, hl = Accuracy_n_HammingScore(pred, y)\n",
    "        batch_dictionary = {\n",
    "            \"loss\" : loss,\n",
    "            \"acc\"  : acc,\n",
    "            \"hl\"   : hl\n",
    "        }\n",
    "        return batch_dictionary\n",
    "                    \n",
    "    \n",
    "def ResNet34(num_classes, channels=3):\n",
    "    return ResNet(Block, [3, 4, 6, 3], num_classes, channels)\n",
    "\n",
    "def ResNet50(num_classes, channels=3):\n",
    "    return ResNet(Bottleneck, [3, 4, 6, 3], num_classes, channels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import Callback\n",
    "\n",
    "class ResNetCallback(Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.train_outputs = []\n",
    "        self.val_outputs = []\n",
    "    \n",
    "    def on_fit_start(self, trainer, pl_module):\n",
    "        pretrained_dict = torch.load('runs/ResNet50_Pretrained_Standardized_exp0/model.pth')\n",
    "        model_dict = pl_module.state_dict()\n",
    "        pretrained_dict = {k:v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "        model_dict.update(pretrained_dict)\n",
    "        pl_module.load_state_dict(model_dict)\n",
    "        \n",
    "    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx, unused = 0):\n",
    "        self.train_outputs.append(outputs)\n",
    "        \n",
    "    def on_validation_batch_end(self, trainer, pl_module, outputs, batch, batch_idx, unused=0):\n",
    "        self.val_outputs.append(outputs)\n",
    "    \n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        avg_loss = torch.stack([x['loss'] for x in self.train_outputs]).mean()\n",
    "        avg_acc  = torch.stack([x['acc'] for x in self.train_outputs]).mean()\n",
    "        avg_hl   = torch.stack([x['hl'] for x in self.train_outputs]).mean()\n",
    "        \n",
    "        pl_module.logger.experiment.add_scalar(\"Loss/train\",\n",
    "                                          avg_loss,\n",
    "                                          pl_module.current_epoch)\n",
    "        \n",
    "        pl_module.logger.experiment.add_scalar(\"Accuracy/train\",\n",
    "                                          avg_acc,\n",
    "                                          pl_module.current_epoch)\n",
    "        \n",
    "        pl_module.logger.experiment.add_scalar(\"HammingScore/train\",\n",
    "                                          avg_hl,\n",
    "                                          pl_module.current_epoch)\n",
    "        \n",
    "        self.ActivationMaps(pl_module, pl_module.reference_image)\n",
    "    \n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        avg_loss = torch.stack([x['loss'] for x in self.val_outputs]).mean()\n",
    "        avg_acc  = torch.stack([x['acc'] for x in self.val_outputs]).mean()\n",
    "        avg_hl   = torch.stack([x['hl'] for x in self.val_outputs]).mean()\n",
    "\n",
    "        #Logging to tensorboard\n",
    "        pl_module.logger.experiment.add_scalar(\"Loss/validation\",\n",
    "                                          avg_loss,\n",
    "                                          pl_module.current_epoch)\n",
    "        \n",
    "        pl_module.logger.experiment.add_scalar(\"Accuracy/validation\",\n",
    "                                          avg_acc,\n",
    "                                          pl_module.current_epoch)\n",
    "        \n",
    "        pl_module.logger.experiment.add_scalar(\"HammingScore/validation\",\n",
    "                                          avg_hl,\n",
    "                                          pl_module.current_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "   | Name        | Type              | Params\n",
      "---------------------------------------------------\n",
      "0  | loss_fn     | BCEWithLogitsLoss | 0     \n",
      "1  | conv1       | Conv2d            | 9.4 K \n",
      "2  | batch_norm1 | BatchNorm2d       | 128   \n",
      "3  | relu        | ReLU              | 0     \n",
      "4  | max_pool    | MaxPool2d         | 0     \n",
      "5  | layer1      | Sequential        | 217 K \n",
      "6  | layer2      | Sequential        | 1.2 M \n",
      "7  | layer3      | Sequential        | 7.1 M \n",
      "8  | layer4      | Sequential        | 15.0 M\n",
      "9  | avgpool     | AdaptiveAvgPool2d | 0     \n",
      "10 | fc          | Linear            | 41.0 K\n",
      "---------------------------------------------------\n",
      "23.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "23.6 M    Total params\n",
      "94.302    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/datnguyenthanh/datnt_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:110: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/datnguyenthanh/datnt_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:110: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/314 [00:00<?, ?it/s] loss: 0.165209, training accuracy: 94.1%, hamming score: 21.9% Batch: 0\n",
      "Epoch 0:  16%|█▌        | 50/314 [00:13<01:13,  3.61it/s, loss=0.195, v_num=2]loss: 0.207234, training accuracy: 92.8%, hamming score: 12.5% Batch: 50\n",
      "Epoch 0:  32%|███▏      | 100/314 [00:27<00:59,  3.59it/s, loss=0.185, v_num=2]loss: 0.172942, training accuracy: 94.1%, hamming score: 32.3% Batch: 100\n",
      "Epoch 0:  48%|████▊     | 150/314 [00:41<00:45,  3.62it/s, loss=0.19, v_num=2] loss: 0.153163, training accuracy: 94.1%, hamming score: 38.5% Batch: 150\n",
      "Epoch 0: 100%|██████████| 314/314 [01:08<00:00,  4.61it/s, loss=0.182, v_num=2]"
     ]
    },
    {
     "ename": "ModuleAttributeError",
     "evalue": "'ResNet' object has no attribute 'reference_image'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleAttributeError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 10\u001b[0m\n\u001b[1;32m      2\u001b[0m logger \u001b[39m=\u001b[39m TensorBoardLogger(\u001b[39m'\u001b[39m\u001b[39mruns\u001b[39m\u001b[39m'\u001b[39m, name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mResNet50_Pretrained_Lightning\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m trainer \u001b[39m=\u001b[39m pl\u001b[39m.\u001b[39mTrainer(accelerator\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgpu\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m                      , max_epochs\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m      5\u001b[0m                      , devices\u001b[39m=\u001b[39m[\u001b[39m1\u001b[39m]\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m                      , callbacks\u001b[39m=\u001b[39m[ResNetCallback()]\n\u001b[1;32m      9\u001b[0m                      )\n\u001b[0;32m---> 10\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(model, voc\u001b[39m.\u001b[39;49mtrain_dataloader(), voc\u001b[39m.\u001b[39;49mval_dataloader())\n",
      "File \u001b[0;32m~/datnt_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:735\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, train_dataloader, ckpt_path)\u001b[0m\n\u001b[1;32m    730\u001b[0m     rank_zero_deprecation(\n\u001b[1;32m    731\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`trainer.fit(train_dataloader)` is deprecated in v1.4 and will be removed in v1.6.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    732\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m Use `trainer.fit(train_dataloaders)` instead. HINT: added \u001b[39m\u001b[39m'\u001b[39m\u001b[39ms\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    733\u001b[0m     )\n\u001b[1;32m    734\u001b[0m     train_dataloaders \u001b[39m=\u001b[39m train_dataloader\n\u001b[0;32m--> 735\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    736\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    737\u001b[0m )\n",
      "File \u001b[0;32m~/datnt_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:682\u001b[0m, in \u001b[0;36mTrainer._call_and_handle_interrupt\u001b[0;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    673\u001b[0m \u001b[39mError handling, intended to be used only for main trainer function entry points (fit, validate, test, predict)\u001b[39;00m\n\u001b[1;32m    674\u001b[0m \u001b[39mas all errors should funnel through them\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[39m    **kwargs: keyword arguments to be passed to `trainer_fn`\u001b[39;00m\n\u001b[1;32m    680\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    681\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 682\u001b[0m     \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    683\u001b[0m \u001b[39m# TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\u001b[39;00m\n\u001b[1;32m    684\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m \u001b[39mas\u001b[39;00m exception:\n",
      "File \u001b[0;32m~/datnt_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:770\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    768\u001b[0m \u001b[39m# TODO: ckpt_path only in v1.7\u001b[39;00m\n\u001b[1;32m    769\u001b[0m ckpt_path \u001b[39m=\u001b[39m ckpt_path \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresume_from_checkpoint\n\u001b[0;32m--> 770\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49mckpt_path)\n\u001b[1;32m    772\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    773\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/datnt_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1193\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheckpoint_connector\u001b[39m.\u001b[39mresume_end()\n\u001b[1;32m   1192\u001b[0m \u001b[39m# dispatch `start_training` or `start_evaluating` or `start_predicting`\u001b[39;00m\n\u001b[0;32m-> 1193\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch()\n\u001b[1;32m   1195\u001b[0m \u001b[39m# plugin will finalized fitting (e.g. ddp_spawn will load trained model)\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_post_dispatch()\n",
      "File \u001b[0;32m~/datnt_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1272\u001b[0m, in \u001b[0;36mTrainer._dispatch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_type_plugin\u001b[39m.\u001b[39mstart_predicting(\u001b[39mself\u001b[39m)\n\u001b[1;32m   1271\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1272\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_type_plugin\u001b[39m.\u001b[39;49mstart_training(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[0;32m~/datnt_venv/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py:202\u001b[0m, in \u001b[0;36mTrainingTypePlugin.start_training\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstart_training\u001b[39m(\u001b[39mself\u001b[39m, trainer: \u001b[39m\"\u001b[39m\u001b[39mpl.Trainer\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    201\u001b[0m     \u001b[39m# double dispatch to initiate the training loop\u001b[39;00m\n\u001b[0;32m--> 202\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_results \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mrun_stage()\n",
      "File \u001b[0;32m~/datnt_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1282\u001b[0m, in \u001b[0;36mTrainer.run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1280\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting:\n\u001b[1;32m   1281\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_predict()\n\u001b[0;32m-> 1282\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_train()\n",
      "File \u001b[0;32m~/datnt_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1312\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1310\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_loop\u001b[39m.\u001b[39mtrainer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\n\u001b[1;32m   1311\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1312\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_loop\u001b[39m.\u001b[39;49mrun()\n",
      "File \u001b[0;32m~/datnt_venv/lib/python3.8/site-packages/pytorch_lightning/loops/base.py:145\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 145\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    146\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    147\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrestarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/datnt_venv/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py:234\u001b[0m, in \u001b[0;36mFitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    231\u001b[0m data_fetcher \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39m_data_connector\u001b[39m.\u001b[39mget_profiled_dataloader(dataloader)\n\u001b[1;32m    233\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mrun_training_epoch\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 234\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loop\u001b[39m.\u001b[39;49mrun(data_fetcher)\n\u001b[1;32m    236\u001b[0m     \u001b[39m# the global step is manually decreased here due to backwards compatibility with existing loggers\u001b[39;00m\n\u001b[1;32m    237\u001b[0m     \u001b[39m# as they expect that the same step is used when logging epoch end metrics even when the batch loop has\u001b[39;00m\n\u001b[1;32m    238\u001b[0m     \u001b[39m# finished. this means the attribute does not exactly track the number of optimizer steps applied.\u001b[39;00m\n\u001b[1;32m    239\u001b[0m     \u001b[39m# TODO(@carmocca): deprecate and rename so users don't get confused\u001b[39;00m\n\u001b[1;32m    240\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/datnt_venv/lib/python3.8/site-packages/pytorch_lightning/loops/base.py:151\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m    149\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m--> 151\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mon_run_end()\n\u001b[1;32m    152\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/datnt_venv/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:302\u001b[0m, in \u001b[0;36mTrainingEpochLoop.on_run_end\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mfit_loop\u001b[39m.\u001b[39mepoch_progress\u001b[39m.\u001b[39mincrement_processed()\n\u001b[1;32m    301\u001b[0m \u001b[39m# call train epoch end hooks\u001b[39;00m\n\u001b[0;32m--> 302\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49mcall_hook(\u001b[39m\"\u001b[39;49m\u001b[39mon_train_epoch_end\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    303\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mcall_hook(\u001b[39m\"\u001b[39m\u001b[39mon_epoch_end\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    304\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mlogger_connector\u001b[39m.\u001b[39mon_epoch_end()\n",
      "File \u001b[0;32m~/datnt_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1477\u001b[0m, in \u001b[0;36mTrainer.call_hook\u001b[0;34m(self, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1475\u001b[0m callback_fx \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, hook_name, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m   1476\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcallable\u001b[39m(callback_fx):\n\u001b[0;32m-> 1477\u001b[0m     callback_fx(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1479\u001b[0m \u001b[39m# next call hook in lightningModule\u001b[39;00m\n\u001b[1;32m   1480\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/datnt_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/callback_hook.py:93\u001b[0m, in \u001b[0;36mTrainerCallbackHookMixin.on_train_epoch_end\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Called when the epoch ends.\"\"\"\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[39mfor\u001b[39;00m callback \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks:\n\u001b[0;32m---> 93\u001b[0m     callback\u001b[39m.\u001b[39;49mon_train_epoch_end(\u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlightning_module)\n",
      "Cell \u001b[0;32mIn[12], line 78\u001b[0m, in \u001b[0;36mResNetCallback.on_train_epoch_end\u001b[0;34m(self, trainer, pl_module)\u001b[0m\n\u001b[1;32m     70\u001b[0m pl_module\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39mexperiment\u001b[39m.\u001b[39madd_scalar(\u001b[39m\"\u001b[39m\u001b[39mAccuracy/train\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     71\u001b[0m                                   avg_acc,\n\u001b[1;32m     72\u001b[0m                                   pl_module\u001b[39m.\u001b[39mcurrent_epoch)\n\u001b[1;32m     74\u001b[0m pl_module\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39mexperiment\u001b[39m.\u001b[39madd_scalar(\u001b[39m\"\u001b[39m\u001b[39mHammingScore/train\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     75\u001b[0m                                   avg_hl,\n\u001b[1;32m     76\u001b[0m                                   pl_module\u001b[39m.\u001b[39mcurrent_epoch)\n\u001b[0;32m---> 78\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mActivationMaps(pl_module, pl_module\u001b[39m.\u001b[39;49mreference_image)\n",
      "File \u001b[0;32m~/datnt_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:778\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m    777\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m--> 778\u001b[0m \u001b[39mraise\u001b[39;00m ModuleAttributeError(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    779\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[0;31mModuleAttributeError\u001b[0m: 'ResNet' object has no attribute 'reference_image'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 314/314 [01:20<00:00,  3.92it/s, loss=0.182, v_num=2]"
     ]
    }
   ],
   "source": [
    "model = ResNet50(20)\n",
    "logger = TensorBoardLogger('runs', name='ResNet50_Pretrained_Lightning')\n",
    "trainer = pl.Trainer(accelerator=\"gpu\"\n",
    "                     , max_epochs=1\n",
    "                     , devices=[1]\n",
    "                     , progress_bar_refresh_rate=1\n",
    "                     , logger=logger\n",
    "                     , callbacks=[ResNetCallback()]\n",
    "                     )\n",
    "trainer.fit(model, voc.train_dataloader(), voc.val_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleImg = torch.rand((1,3,300,300))\n",
    "logger.experiment.add_graph(model, sampleImg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makegrid(output, numrows):\n",
    "    outer = (torch.Tensor.cpu(output).detach())\n",
    "    plt.figure(figsize=(20,5))\n",
    "    b = np.array([]).reshape(0, outer.shape[2])\n",
    "    c = np.array([]).reshape(numrows*outer.shape[2], 0)\n",
    "    i=0\n",
    "    j=0\n",
    "    while(i<outer.shape[1]):\n",
    "        img = outer[0][i]\n",
    "        b = np.concatenate((img,b), axis=0)\n",
    "        j += 1\n",
    "        if (j == numrows):\n",
    "            c = np.concatenate((c,b), axis=1)\n",
    "            b = np.array([]).reshape(0, outer.shape[2])\n",
    "            j = 0\n",
    "        i += 1\n",
    "    return c\n",
    "\n",
    "def ActivationMaps(pl_module, x, device = 1):\n",
    "        pl_module.logger.experiment.add_image(\"input\", torch.Tensor.cpu(x[0]), pl_module.current_epoch, dataformats=\"CHW\")\n",
    "        \n",
    "        out = pl_module.relu(pl_module.batch_norm1(pl_module.conv1(x.to(f\"cuda:{device}\"))))\n",
    "        out = pl_module.max_pool(out)\n",
    "        c = makegrid(out, 4)\n",
    "        pl_module.logger.experiment.add_image(\"layer0\", c, pl_module.current_epoch, dataformats=\"HW\")\n",
    "        \n",
    "        out = pl_module.layer1(out)\n",
    "        c = makegrid(out, 4)\n",
    "        pl_module.logger.experiment.add_image(\"layer1\", c, pl_module.current_epoch, dataformats=\"HW\")\n",
    "        \n",
    "        out = pl_module.layer2(out)\n",
    "        c = makegrid(out, 8)\n",
    "        pl_module.logger.experiment.add_image(\"layer2\", c, pl_module.current_epoch, dataformats=\"HW\")\n",
    "\n",
    "        out = pl_module.layer3(out)\n",
    "        c = makegrid(out, 8)\n",
    "        pl_module.logger.experiment.add_image(\"layer3\", c, pl_module.current_epoch, dataformats=\"HW\")\n",
    "\n",
    "        out = pl_module.layer4(out)\n",
    "        c = makegrid(out, 8)\n",
    "        pl_module.logger.experiment.add_image(\"layer4\", c, pl_module.current_epoch, dataformats=\"HW\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.7094, -1.7094, -1.7094,  ..., -1.7094, -1.7094, -1.7094],\n",
       "         [-1.7094, -1.7094, -1.7094,  ..., -1.7094, -1.7094, -1.7094],\n",
       "         [-1.7094, -1.7094, -1.7094,  ..., -1.7094, -1.7094, -1.7094],\n",
       "         ...,\n",
       "         [-1.7094, -1.7094, -1.7094,  ..., -1.7094, -1.7094, -1.7094],\n",
       "         [-1.7094, -1.7094, -1.7094,  ..., -1.7094, -1.7094, -1.7094],\n",
       "         [-1.7094, -1.7094, -1.7094,  ..., -1.7094, -1.7094, -1.7094]],\n",
       "\n",
       "        [[-1.6632, -1.6632, -1.6632,  ..., -1.6632, -1.6632, -1.6632],\n",
       "         [-1.6632, -1.6632, -1.6632,  ..., -1.6632, -1.6632, -1.6632],\n",
       "         [-1.6632, -1.6632, -1.6632,  ..., -1.6632, -1.6632, -1.6632],\n",
       "         ...,\n",
       "         [-1.6632, -1.6632, -1.6632,  ..., -1.6632, -1.6632, -1.6632],\n",
       "         [-1.6632, -1.6632, -1.6632,  ..., -1.6632, -1.6632, -1.6632],\n",
       "         [-1.6632, -1.6632, -1.6632,  ..., -1.6632, -1.6632, -1.6632]],\n",
       "\n",
       "        [[-1.4669, -1.4669, -1.4669,  ..., -1.4669, -1.4669, -1.4669],\n",
       "         [-1.4669, -1.4669, -1.4669,  ..., -1.4669, -1.4669, -1.4669],\n",
       "         [-1.4669, -1.4669, -1.4669,  ..., -1.4669, -1.4669, -1.4669],\n",
       "         ...,\n",
       "         [-1.4669, -1.4669, -1.4669,  ..., -1.4669, -1.4669, -1.4669],\n",
       "         [-1.4669, -1.4669, -1.4669,  ..., -1.4669, -1.4669, -1.4669],\n",
       "         [-1.4669, -1.4669, -1.4669,  ..., -1.4669, -1.4669, -1.4669]]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_img = next(iter(voc.train_dataloader()))[0]\n",
    "sample_img[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datnt_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
